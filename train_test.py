import os
import time
from datetime import datetime
import numpy as np
from scipy import sparse
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import KFold
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset

# ç¡®ä¿è¿™äº›è‡ªå®šä¹‰æ¨¡å—å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥
from data_process import min_seq_len, max_seq_len
from dataset import UserDataset
from sqgkt import sqgkt
from params import *
from utils import gen_sqgkt_graph, build_adj_list, build_adj_list_uq, gen_sqgkt_graph_uq

# --- 1. åˆå§‹åŒ–è®¾ç½® ---
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
time_now = datetime.now().strftime('%Y_%m_%d#%H_%M_%S')
output_path = os.path.join("output", time_now)
os.makedirs(output_path, exist_ok=True)
output_file_path = os.path.join(output_path, "log.txt")
output_file = open(output_file_path, "w")

# --- 2. è¶…å‚æ•°é…ç½® ---
params = {
    'max_seq_len': max_seq_len,
    'min_seq_len': min_seq_len,
    'epochs': 40,
    'lr': 0.01,
    'lr_gamma': 0.85,
    'batch_size': 128,
    'size_q_neighbors': 4,
    'size_q_neighbors_2': 5,
    'size_s_neighbors': 10,
    'size_u_neighbors': 5,
    'num_workers': 0,
    'prefetch_factor': 4,
    'agg_hops': 3,
    'emb_dim': 100,
    'hard_recap': False,
    'dropout': (0.2, 0.4),
    'rank_k': 10,
    'k_fold': 5
}
output_file.write(str(params) + '\n')
print(params)

# --- 3. æ•°æ®åŠ è½½ä¸å›¾æ„å»º ---
qs_table = torch.tensor(sparse.load_npz('data/qs_table.npz').toarray(), dtype=torch.int64, device=DEVICE)
uq_table = torch.tensor(np.load('data/uq_table.npy'), dtype=torch.float32, device=DEVICE)

num_question = torch.tensor(qs_table.shape[0], device=DEVICE)
num_skill = torch.tensor(qs_table.shape[1], device=DEVICE)
num_user = torch.tensor(uq_table.shape[0], device=DEVICE)

q_neighbors_list, s_neighbors_list = build_adj_list()
q_neighbors, s_neighbors = gen_sqgkt_graph(q_neighbors_list, s_neighbors_list, params['size_q_neighbors'],
                                           params['size_s_neighbors'])
q_neighbors = torch.tensor(q_neighbors, dtype=torch.int64, device=DEVICE)
s_neighbors = torch.tensor(s_neighbors, dtype=torch.int64, device=DEVICE)

u_neighbors_list, q_neighbors_list = build_adj_list_uq()
u_neighbors, q_neighbors_2 = gen_sqgkt_graph_uq(u_neighbors_list, q_neighbors_list, params['size_u_neighbors'],
                                                params['size_q_neighbors_2'])
u_neighbors = torch.tensor(u_neighbors, dtype=torch.int64, device=DEVICE)
q_neighbors_2 = torch.tensor(q_neighbors_2, dtype=torch.int64, device=DEVICE)

# --- 4. æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨åˆå§‹åŒ– ---
model = sqgkt(
    num_question, num_skill, q_neighbors, s_neighbors, qs_table, num_user, u_neighbors, q_neighbors_2, uq_table,
    agg_hops=params['agg_hops'],
    emb_dim=params['emb_dim'],
    dropout=params['dropout'],
    hard_recap=params['hard_recap'],
).to(DEVICE)

loss_fun = torch.nn.BCEWithLogitsLoss().to(DEVICE)
optimizer = torch.optim.Adam(params=model.parameters(), lr=params['lr'])
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, params['lr_gamma'])

# --- 5. æ–­ç‚¹ç»­è®­é€»è¾‘ ---
resume_path = "model/checkpoint_last.pt"
start_epoch = 0
max_auc = 0.0

if os.path.exists(resume_path):
    print(f"âœ… Resuming from checkpoint: {resume_path}")
    checkpoint = torch.load(resume_path, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    max_auc = checkpoint['max_auc']
    print(f"â–¶ï¸ Resumed from epoch {start_epoch}, current max_auc is {max_auc:.4f}")
else:
    print("â„¹ï¸ No checkpoint found, starting from scratch.")

dataset = UserDataset()
print('model has been built')

# --- 6. è®­ç»ƒä¸è¯„ä¼°å¾ªç¯ ---
k_fold = KFold(n_splits=params['k_fold'], shuffle=True, random_state=42)

# epoch_total_counter å·²è¢«ç§»é™¤ï¼Œæˆ‘ä»¬ç”¨ epoch å’Œ fold ç›´æ¥æ˜¾ç¤º
for epoch in range(start_epoch, params['epochs']):
    train_loss_aver, train_acc_aver, train_auc_aver = 0.0, 0.0, 0.0
    test_loss_aver, test_acc_aver, test_auc_aver = 0.0, 0.0, 0.0

    # å†…å±‚ K-Fold å¾ªç¯
    for fold, (train_indices, test_indices) in enumerate(k_fold.split(dataset)):
        time0 = time.time()
        # epoch_total_counter += 1
        print(
            '===================' + LOG_Y + f' Epoch: {epoch + 1}/{params["epochs"]}, Fold: {fold + 1}/{params["k_fold"]} ' + LOG_END + '=====================')

        train_set = Subset(dataset, train_indices)
        test_set = Subset(dataset, test_indices)
        train_data_len, test_data_len = len(train_set), len(test_set)

        train_loader = DataLoader(train_set, batch_size=params['batch_size'], num_workers=params['num_workers'],
                                  pin_memory=True, shuffle=True)
        test_loader = DataLoader(test_set, batch_size=params['batch_size'], num_workers=params['num_workers'],
                                 pin_memory=True)

        # --- Training Phase ---
        model.train()
        train_step, train_loss_total, train_right, train_total, train_auc_total = 0, 0.0, 0.0, 0.0, 0.0
        print('-------------------training------------------')
        for data in train_loader:
            optimizer.zero_grad()
            u, x, y_target, mask = (d.to(DEVICE) for d in
                                    [data[:, :, 0], data[:, :, 1], data[:, :, 2], data[:, :, 3].to(torch.bool)])

            y_hat = model(u, x, y_target, mask)
            y_hat_masked = torch.masked_select(y_hat, mask)
            y_target_masked = torch.masked_select(y_target, mask)

            loss = loss_fun(y_hat_masked, y_target_masked.to(torch.float32))
            loss.backward()
            optimizer.step()

            y_pred_masked = (y_hat_masked >= 0.0).int()

            # --- å®æ—¶è®¡ç®—ä¸æ‰“å° ---
            train_step += 1
            batch_loss = loss.item()
            num_valid = torch.sum(mask).item()
            batch_acc = torch.sum(y_pred_masked == y_target_masked).item() / num_valid
            try:
                batch_auc = roc_auc_score(y_target_masked.cpu().numpy(), y_hat_masked.detach().cpu().numpy())
            except ValueError:
                batch_auc = 0.5

            print(f'step: {train_step}, loss: {batch_loss:.4f}, acc: {batch_acc:.4f}, auc: {batch_auc:.4f}')

            # --- ç´¯åŠ ç”¨äº fold å¹³å‡ ---
            train_loss_total += batch_loss
            train_right += torch.sum(y_pred_masked == y_target_masked).item()
            train_total += num_valid
            train_auc_total += batch_auc * num_valid

        fold_train_loss = train_loss_total / train_step
        fold_train_acc = train_right / train_total
        fold_train_auc = train_auc_total / train_total
        train_loss_aver += fold_train_loss
        train_acc_aver += fold_train_acc
        train_auc_aver += fold_train_auc

        # --- Testing Phase ---
        model.eval()
        test_step, test_loss_total, test_right, test_total, test_auc_total = 0, 0.0, 0.0, 0.0, 0.0
        print('-------------------testing------------------')
        with torch.no_grad():
            for data in test_loader:
                u, x, y_target, mask = (d.to(DEVICE) for d in
                                        [data[:, :, 0], data[:, :, 1], data[:, :, 2], data[:, :, 3].to(torch.bool)])

                y_hat = model(u, x, y_target, mask)
                y_hat_masked = torch.masked_select(y_hat, mask)
                y_target_masked = torch.masked_select(y_target, mask)

                loss = loss_fun(y_hat_masked, y_target_masked.to(torch.float32))

                y_pred_masked = (y_hat_masked >= 0.0).int()

                # --- å®æ—¶è®¡ç®—ä¸æ‰“å° ---
                test_step += 1
                batch_loss = loss.item()
                num_valid = torch.sum(mask).item()
                batch_acc = torch.sum(y_pred_masked == y_target_masked).item() / num_valid
                try:
                    batch_auc = roc_auc_score(y_target_masked.cpu().numpy(), y_hat_masked.detach().cpu().numpy())
                except ValueError:
                    batch_auc = 0.5

                print(f'step: {test_step}, loss: {batch_loss:.4f}, acc: {batch_acc:.4f}, auc: {batch_auc:.4f}')

                # --- ç´¯åŠ ç”¨äº fold å¹³å‡ ---
                test_loss_total += batch_loss
                test_right += torch.sum(y_pred_masked == y_target_masked).item()
                test_total += num_valid
                test_auc_total += batch_auc * num_valid

        fold_test_loss = test_loss_total / test_step
        fold_test_acc = test_right / test_total
        fold_test_auc = test_auc_total / test_total

        test_loss_aver += fold_test_loss
        test_acc_aver += fold_test_acc
        test_auc_aver += fold_test_auc

        run_time = time.time() - time0
        print(
            LOG_B + f'Fold Training:   loss: {fold_train_loss:.4f}, acc: {fold_train_acc:.4f}, auc: {fold_train_auc:.4f}' + LOG_END)
        print(
            LOG_B + f'Fold Testing:    loss: {fold_test_loss:.4f}, acc: {fold_test_acc:.4f}, auc: {fold_test_auc:.4f}' + LOG_END)
        print(LOG_B + f'Time: {run_time:.2f}s' + LOG_END)

        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        output_file.write(
            f'epoch {epoch + 1}, fold {fold + 1} | train_loss: {fold_train_loss:.4f}, train_acc: {fold_train_acc:.4f}, train_auc: {fold_train_auc:.4f} | test_loss: {fold_test_loss:.4f}, test_acc: {fold_test_acc:.4f}, test_auc: {fold_test_auc:.4f}\n')
        # å®æ—¶åˆ·æ–°ç¼“å†²åŒºï¼Œç¡®ä¿æ—¥å¿—ä¸ä¸¢å¤±
        output_file.flush()

    # --- K-Fold å¾ªç¯ç»“æŸï¼Œè®¡ç®—å¹³å‡å€¼å¹¶ä¿å­˜ ---
    train_loss_aver /= params['k_fold']
    train_acc_aver /= params['k_fold']
    train_auc_aver /= params['k_fold']
    test_loss_aver /= params['k_fold']
    test_acc_aver /= params['k_fold']
    test_auc_aver /= params['k_fold']

    print('>>>>>>>>>>>>>>>>>>' + LOG_Y + f" Epoch: {epoch + 1} Average Results " + LOG_END + '<<<<<<<<<<<<<<<<<<')
    print(
        LOG_G + f'Training Average: loss: {train_loss_aver:.4f}, acc: {train_acc_aver:.4f}, auc: {train_auc_aver:.4f}' + LOG_END)
    print(
        LOG_G + f'Testing Average:  loss: {test_loss_aver:.4f}, acc: {test_acc_aver:.4f}, auc: {test_auc_aver:.4f}' + LOG_END)

    output_file.write(
        f'--- epoch {epoch + 1} average | train_loss: {train_loss_aver:.4f}, train_acc: {train_acc_aver:.4f}, train_auc: {train_auc_aver:.4f} | test_loss: {test_loss_aver:.4f}, test_acc: {test_acc_aver:.4f}, test_auc: {test_auc_aver:.4f} ---\n')

    # 1. æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
    if test_auc_aver > max_auc:
        max_auc = test_auc_aver
        print(f"ğŸ‰ New best model found with average AUC: {max_auc:.4f}. Saving to model/model_best_auc.pt")
        os.makedirs('model', exist_ok=True)
        torch.save(model.state_dict(), 'model/model_best_auc.pt')

    # 2. ä¿å­˜æœ€æ–°çš„æ£€æŸ¥ç‚¹
    print(f"ğŸ’¾ Saving last checkpoint for epoch {epoch} to model/checkpoint_last.pt")
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'max_auc': max_auc,
    }, 'model/checkpoint_last.pt')

    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step()

output_file.close()
print("ğŸ‰ Training finished!")