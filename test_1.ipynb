{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-28T09:44:11.825677Z",
     "start_time": "2025-09-28T09:44:11.223757Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import os\n",
    "\n",
    "# 导入你的项目中的必要模块\n",
    "# 确保这些路径是正确的，如果需要，请根据你的项目结构调整\n",
    "from params import DEVICE # 假设 DEVICE 已在 params.py 中定义\n",
    "from data_process import min_seq_len, max_seq_len # 假设 min_seq_len, max_seq_len 已在 data_process.py 中定义\n",
    "from utils import gen_sqgkt_graph, build_adj_list, build_adj_list_uq, gen_sqgkt_graph_uq\n",
    "\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 模拟 train_test.py 中的 params\n",
    "params = {\n",
    "    'max_seq_len': max_seq_len,\n",
    "    'min_seq_len': min_seq_len,\n",
    "    'epochs': 2,\n",
    "    'lr': 0.01,\n",
    "    'lr_gamma': 0.85,\n",
    "    'batch_size': 128,\n",
    "    'size_q_neighbors': 4,\n",
    "    'size_q_neighbors_2': 5, # 你的 u_neighbors 关联的 q_neighbors_2\n",
    "    'size_s_neighbors': 10,\n",
    "    'size_u_neighbors': 5,  # 你的用户邻居（问题）的最大数量\n",
    "    'num_workers': 0,\n",
    "    'prefetch_factor': 4,\n",
    "    'agg_hops': 3,\n",
    "    'emb_dim': 100,\n",
    "    'hard_recap': False,\n",
    "    'dropout': (0.2, 0.4),\n",
    "    'rank_k': 10,\n",
    "    'k_fold': 5\n",
    "}\n",
    "print(f\"Loaded parameters: {params}\")\n",
    "\n",
    "# --- 1. 原始 uq_table.npz 文件加载后查看形状 ---\n",
    "print(\"\\n--- Debugging uq_table loading ---\")\n",
    "\n",
    "# 加载 qs_table (通常是 (num_question, num_skill))\n",
    "qs_table_np = sparse.load_npz('data/qs_table.npz').toarray()\n",
    "qs_table = torch.tensor(qs_table_np, dtype=torch.int64, device=DEVICE)\n",
    "num_question_total = qs_table_np.shape[0] # 获取总问题数，后续用于检查 question_id 范围\n",
    "\n",
    "print(f\"Shape of qs_table_np (numpy array): {qs_table_np.shape}\")\n",
    "print(f\"Shape of qs_table (torch.Tensor): {qs_table.shape}\")\n",
    "\n",
    "\n",
    "# 加载原始 uq_table 文件\n",
    "raw_uq_matrix_np = sparse.load_npz('data/uq_table.npz').toarray()\n",
    "print(f\"DEBUG 1: Shape of raw_uq_matrix_np (numpy array from uq_table.npz): {raw_uq_matrix_np.shape}\")\n",
    "\n",
    "# 原始 uq_table 直接转换为 tensor 的形状 (这将是二维的)\n",
    "uq_table_initial_tensor = torch.tensor(raw_uq_matrix_np, dtype=torch.int64, device=DEVICE)\n",
    "print(f\"DEBUG 2: Shape of uq_table_initial_tensor (torch.Tensor, direct conversion): {uq_table_initial_tensor.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. 模拟构建 u_neighbors 和 q_neighbors_2 ---\n",
    "# 这些图结构是构建三维 uq_table 的必要前提\n",
    "print(\"\\n--- Simulating u_neighbors and q_neighbors_2 generation ---\")\n",
    "u_neighbors_list, q_neighbors_list_for_uq = build_adj_list_uq() # 注意这里q_neighbors_list是用于uq的\n",
    "u_neighbors_np, q_neighbors_2_np = gen_sqgkt_graph_uq(u_neighbors_list, q_neighbors_list_for_uq,\n",
    "                                                      params['size_u_neighbors'], params['size_q_neighbors_2'])\n",
    "u_neighbors = torch.tensor(u_neighbors_np, dtype=torch.int64, device=DEVICE)\n",
    "q_neighbors_2 = torch.tensor(q_neighbors_2_np, dtype=torch.int64, device=DEVICE)\n",
    "\n",
    "print(f\"DEBUG 3: Shape of u_neighbors (torch.Tensor): {u_neighbors.shape}\")\n",
    "print(f\"DEBUG 4: Shape of q_neighbors_2 (torch.Tensor): {q_neighbors_2.shape}\")\n",
    "\n",
    "# --- 3. 模拟构建三维 uq_table ---\n",
    "print(\"\\n--- Simulating 3D uq_table construction ---\")\n",
    "\n",
    "# 获取必要的维度信息\n",
    "num_user_from_data = raw_uq_matrix_np.shape[0] # 从原始数据获取用户数\n",
    "max_user_neighbors = params['size_u_neighbors'] # 从 params 中获取每个用户的最大邻居数\n",
    "\n",
    "print(f\"DEBUG 5: Derived num_user_from_data: {num_user_from_data}\")\n",
    "print(f\"DEBUG 6: Derived max_user_neighbors (from params): {max_user_neighbors}\")\n",
    "\n",
    "# 创建一个三维的 uq_table 张量，零初始化\n",
    "processed_uq_table_data = torch.zeros(num_user_from_data, max_user_neighbors, 3, dtype=torch.float32)\n",
    "print(f\"DEBUG 7: Shape of processed_uq_table_data (zero-initialized, expected 3D): {processed_uq_table_data.shape}\")\n",
    "\n",
    "\n",
    "# --- 4. 填充 processed_uq_table_data (关键逻辑，请根据你的论文和数据完善) ---\n",
    "print(\"\\n--- Populating 3D uq_table data ---\")\n",
    "# 遍历每个用户\n",
    "for user_idx in range(num_user_from_data):\n",
    "    # 获取当前用户的邻居问题列表\n",
    "    current_user_neighbor_questions = u_neighbors[user_idx].cpu().numpy()\n",
    "\n",
    "    # 遍历当前用户的每个邻居问题槽位\n",
    "    for neighbor_slot_idx, question_id in enumerate(current_user_neighbor_questions):\n",
    "        # 检查 question_id 是否有效 (非填充值，且在问题总数范围内)\n",
    "        if question_id != -1 and question_id >= 0 and question_id < num_question_total:\n",
    "            # --- 这是你需要根据论文和数据定制的逻辑 ---\n",
    "            # 假设 raw_uq_matrix_np 是一个 (num_user, num_question_total) 矩阵，\n",
    "            # 存储了用户 `user_idx` 和问题 `question_id` 之间的连接强度。\n",
    "\n",
    "            connection_strength = raw_uq_matrix_np[user_idx, question_id]\n",
    "\n",
    "            # 示例逻辑：\n",
    "            c_i_val = float(connection_strength) # 例如，用原始连接强度作为 c_i\n",
    "            g_p_val = 0.5 if connection_strength > 0 else 0.0 # 例如，如果有连接，给一个正向增益\n",
    "            g_n_val = 0.5 if connection_strength > 0 else 0.0 # 例如，如果有连接，给一个负向增益\n",
    "\n",
    "            # 赋值到 processed_uq_table_data\n",
    "            processed_uq_table_data[user_idx, neighbor_slot_idx, 0] = c_i_val\n",
    "            processed_uq_table_data[user_idx, neighbor_slot_idx, 1] = g_p_val\n",
    "            processed_uq_table_data[user_idx, neighbor_slot_idx, 2] = g_n_val\n",
    "# --- 填充结束 ---\n",
    "\n",
    "# 将填充好的 processed_uq_table_data 移动到指定设备 (GPU/CPU)\n",
    "uq_table_final_tensor = processed_uq_table_data.to(DEVICE)\n",
    "print(f\"DEBUG 8: Final uq_table_final_tensor shape after population: {uq_table_final_tensor.shape}\")\n",
    "\n",
    "print(\"\\n--- Debugging complete ---\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loaded parameters: {'max_seq_len': 200, 'min_seq_len': 20, 'epochs': 2, 'lr': 0.01, 'lr_gamma': 0.85, 'batch_size': 128, 'size_q_neighbors': 4, 'size_q_neighbors_2': 5, 'size_s_neighbors': 10, 'size_u_neighbors': 5, 'num_workers': 0, 'prefetch_factor': 4, 'agg_hops': 3, 'emb_dim': 100, 'hard_recap': False, 'dropout': (0.2, 0.4), 'rank_k': 10, 'k_fold': 5}\n",
      "\n",
      "--- Debugging uq_table loading ---\n",
      "Shape of qs_table_np (numpy array): (17622, 123)\n",
      "Shape of qs_table (torch.Tensor): torch.Size([17622, 123])\n",
      "DEBUG 1: Shape of raw_uq_matrix_np (numpy array from uq_table.npz): (2131, 17622)\n",
      "DEBUG 2: Shape of uq_table_initial_tensor (torch.Tensor, direct conversion): torch.Size([2131, 17622])\n",
      "\n",
      "--- Simulating u_neighbors and q_neighbors_2 generation ---\n",
      "DEBUG 3: Shape of u_neighbors (torch.Tensor): torch.Size([2131, 5])\n",
      "DEBUG 4: Shape of q_neighbors_2 (torch.Tensor): torch.Size([17622, 5])\n",
      "\n",
      "--- Simulating 3D uq_table construction ---\n",
      "DEBUG 5: Derived num_user_from_data: 2131\n",
      "DEBUG 6: Derived max_user_neighbors (from params): 5\n",
      "DEBUG 7: Shape of processed_uq_table_data (zero-initialized, expected 3D): torch.Size([2131, 5, 3])\n",
      "\n",
      "--- Populating 3D uq_table data ---\n",
      "DEBUG 8: Final uq_table_final_tensor shape after population: torch.Size([2131, 5, 3])\n",
      "\n",
      "--- Debugging complete ---\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T11:01:38.277297Z",
     "start_time": "2025-09-28T11:01:37.809356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import sparse\n",
    "from scipy.stats import norm, poisson\n",
    "\n",
    "min_seq_len = 20\n",
    "max_seq_len = 200\n",
    "k = 0.3\n",
    "d = 0.7\n",
    "b = 10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv(filepath_or_buffer='data/assist09_origin.csv', encoding=\"ISO-8859-1\")\n",
    "    data = data.sort_values(by='user_id', ascending=True)\n",
    "    data = data.drop(data[data['skill_id'] == ' '].index)\n",
    "    data = data.dropna(subset=['skill_id'])\n",
    "    data = data.drop(data[data['original'] == 0].index)\n",
    "    is_valid_user = data.groupby('user_id').size() >= min_seq_len\n",
    "    data = data[data['user_id'].isin(is_valid_user[is_valid_user].index)]\n",
    "    data = data.loc[:, ['order_id', 'user_id', 'problem_id', 'correct', 'skill_id', 'skill_name',\n",
    "                        'ms_first_response', 'answer_type', 'attempt_count','hint_count']]\n",
    "    question_attempt_stats = data.groupby('problem_id')['attempt_count'].mean().reset_index()\n",
    "    question_attempt_stats.rename(columns={'attempt_count': 'mean_attempt'}, inplace=True)\n",
    "    data = pd.merge(data, question_attempt_stats, on='problem_id', suffixes=('', '_attempt'))\n",
    "    data['attempt_factor'] = 1 - poisson(data['mean_attempt']).cdf(data['attempt_count'] - 1)\n",
    "    data['attempt_factor_g'] = k + (1 - k) / (1 + np.exp(-d * (data['attempt_factor'] - b)))\n",
    "\n",
    "\n",
    "    question_hint_stats = data.groupby('problem_id')['hint_count'].agg('mean').reset_index()\n",
    "    question_hint_stats.rename(columns={'hint_count': 'mean_hint'}, inplace=True)\n",
    "    data = pd.merge(data, question_hint_stats, on='problem_id')\n",
    "\n",
    "    # data['hint_count'] = data['hint_count']\n",
    "    #CDF\n",
    "    data['hint_factor'] = 1 - poisson(data['mean_hint']).cdf(data['hint_count'] - 1)\n",
    "    data['hint_factor_g'] = k + (1 - k) / (1 + np.exp(-d * (data['hint_factor'] - b)))\n",
    "    data['ability_factor'] = data.groupby('user_id')['correct'].transform('mean')\n",
    "\n",
    "    num_answer = data.shape[0]\n",
    "    questions = set()\n",
    "    skills = set()\n",
    "    users = set()\n",
    "\n",
    "    for row in data.itertuples(index=False):\n",
    "        users.add(row[1])\n",
    "        questions.add(row[2])\n",
    "        if isinstance(row[4], (int, float)):\n",
    "            skills.add(int(row[4]))\n",
    "        else:\n",
    "            skill_add = set(int(s) for s in row[4].split('_'))\n",
    "            skills = skills.union(skill_add)\n",
    "    data.to_csv('data/data_processed.csv', sep=',', index=False)\n",
    "\n",
    "    num_q = len(questions)\n",
    "    num_s = len(skills)\n",
    "    num_user = len(users)\n",
    "    if not os.path.exists('data/question2idx.npy'):\n",
    "\n",
    "        questions = list(questions)\n",
    "        skills = list(skills)\n",
    "        users = list(users)\n",
    "        question2idx = {questions[i]: i + 1 for i in range(num_q)}\n",
    "        question2idx[0] = 0\n",
    "        skill2idx = {skills[i]: i for i in range(num_s)}\n",
    "        user2idx = {users[i]: i for i in range(num_user)}\n",
    "        num_q += 1\n",
    "        idx2question = {question2idx[q]: q for q in question2idx}\n",
    "        idx2skill = {skill2idx[s]: s for s in skill2idx}\n",
    "        idx2user = {user2idx[u]: u for u in user2idx}\n",
    "\n",
    "        np.save('data/question2idx.npy', question2idx)\n",
    "        np.save('data/skill2idx.npy', skill2idx)\n",
    "        np.save('data/user2idx.npy', user2idx)\n",
    "        np.save('data/idx2question.npy', idx2question)\n",
    "        np.save('data/idx2skill.npy', idx2skill)\n",
    "        np.save('data/idx2user.npy', idx2user)\n",
    "    else:\n",
    "        question2idx = np.load('data/question2idx.npy', allow_pickle=True).item()\n",
    "        skill2idx = np.load('data/skill2idx.npy', allow_pickle=True).item()\n",
    "        user2idx = np.load('data/user2idx.npy', allow_pickle=True).item()\n",
    "        idx2question = np.load('data/idx2question.npy', allow_pickle=True).item()\n",
    "        idx2skill = np.load('data/idx2skill.npy', allow_pickle=True).item()\n",
    "        idx2user = np.load('data/idx2user.npy', allow_pickle=True).item()\n",
    "\n",
    "    #row[1]:user_id, row[2]:problem_id, row[4]:skill_id\n",
    "    if not os.path.exists('data/qs_table.npz'):\n",
    "        qs_table = np.zeros([num_q, num_s], dtype=float)\n",
    "        q_set = data['problem_id'].drop_duplicates()\n",
    "        q_samples = pd.concat([data[data['problem_id'] == q_id].sample(1) for q_id in q_set])\n",
    "        for row in q_samples.itertuples(index=False):\n",
    "            if isinstance(row[4], (int, float)):\n",
    "                qs_table[question2idx[row[2]], skill2idx[int(row[4])]] = 1\n",
    "            else:\n",
    "                skill_add = [int(s) for s in row[4].split('_')]\n",
    "                for s in skill_add:\n",
    "                    qs_table[question2idx[row[2]], skill2idx[s]] = 1\n",
    "        qq_table = np.matmul(qs_table, qs_table.T)\n",
    "        ss_table = np.matmul(qs_table.T, qs_table)\n",
    "        qs_table = sparse.coo_matrix(qs_table)\n",
    "        qq_table = sparse.coo_matrix(qq_table)\n",
    "        ss_table = sparse.coo_matrix(ss_table)\n",
    "        sparse.save_npz('data/qs_table.npz', qs_table)\n",
    "        sparse.save_npz('data/qq_table.npz', qq_table)\n",
    "        sparse.save_npz('data/ss_table.npz', ss_table)\n",
    "    else:\n",
    "        qs_table = sparse.load_npz('data/qs_table.npz').toarray()\n",
    "        qq_table = sparse.load_npz('data/qq_table.npz').toarray()\n",
    "        ss_table = sparse.load_npz('data/ss_table.npz').toarray()\n",
    "    weights = np.array([0.4, 0.4, 0.2])\n",
    "    if not os.path.exists('data/uq_table.npz'):\n",
    "        uq_table = np.zeros([num_user, num_q], dtype=float)\n",
    "        u_set = data['user_id'].drop_duplicates()\n",
    "        u_samples = pd.concat([data[data['user_id'] == u_id].sample(1) for u_id in u_set])\n",
    "        for row in u_samples.itertuples(index=False):\n",
    "            user_index = user2idx[row[1]]\n",
    "            question_index = question2idx[row[2]]\n",
    "            factors = np.array([row.attempt_factor_g, row.hint_factor_g, row.ability_factor])\n",
    "            factor_value = np.sum(factors * weights)\n",
    "            uq_table[user2idx[row[1]], question2idx[(row[2])]] = factor_value\n",
    "        uq_table = sparse.coo_matrix(uq_table)\n",
    "        sparse.save_npz('data/uq_table.npz', uq_table)\n",
    "    else:\n",
    "        uq_table = sparse.load_npz('data/uq_table.npz').toarray()\n",
    "\n",
    "    if not os.path.exists('data/user_seq.npy'):\n",
    "        user_seq = np.zeros([num_user, max_seq_len])\n",
    "        user_res = np.zeros([num_user, max_seq_len])\n",
    "        user_user = np.zeros([num_user, max_seq_len])\n",
    "        num_seq = [0 for _ in range(num_user)]\n",
    "        user_mask = np.zeros([num_user, max_seq_len])\n",
    "        for row in data.itertuples(index=False):\n",
    "            user_id = user2idx[row[1]]\n",
    "            if num_seq[user_id] < max_seq_len - 1:\n",
    "                user_seq[user_id, num_seq[user_id]] = question2idx[row[2]]\n",
    "                user_res[user_id, num_seq[user_id]] = row[3]\n",
    "                user_mask[user_id, num_seq[user_id]] = 1\n",
    "                user_user[user_id, num_seq[user_id]] = user_id\n",
    "                num_seq[user_id] += 1\n",
    "        np.save('data/user_seq.npy', user_seq)\n",
    "        np.save('data/user_res.npy', user_res)\n",
    "        np.save('data/user_mask.npy', user_mask)\n",
    "        np.save('data/user_user.npy', user_mask)"
   ],
   "id": "8a8308981586fd1a",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/assist09_origin.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m b \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 14\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata/assist09_origin.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mISO-8859-1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser_id\u001B[39m\u001B[38;5;124m'\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     16\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mdrop(data[data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mskill_id\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\GCN\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\GCN\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\GCN\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\GCN\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\GCN\\lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/assist09_origin.csv'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:45:19.784594Z",
     "start_time": "2025-09-28T12:45:19.685765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检查uq_table数据\n",
    "print(\"=== uq_table数据检查 ===\")\n",
    "print(f\"uq_table形状: {uq_table_3d.shape}\")\n",
    "print(f\"零值比例: {torch.sum(uq_table_3d == 0) / uq_table_3d.numel():.4f}\")\n",
    "print(f\"数值范围: [{uq_table_3d.min():.4f}, {uq_table_3d.max():.4f}]\")\n",
    "print(f\"各通道均值: {torch.mean(uq_table_3d, dim=(0,1))}\")\n",
    "\n",
    "# 如果发现数据全为0，需要检查数据加载和预处理过程"
   ],
   "id": "e8bbe2ae405bc5f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== uq_table数据检查 ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'uq_table_3d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 检查uq_table数据\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=== uq_table数据检查 ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muq_table形状: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muq_table_3d\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m零值比例: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch\u001B[38;5;241m.\u001B[39msum(uq_table_3d\u001B[38;5;250m \u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39muq_table_3d\u001B[38;5;241m.\u001B[39mnumel()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m数值范围: [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muq_table_3d\u001B[38;5;241m.\u001B[39mmin()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muq_table_3d\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'uq_table_3d' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b90a703645a3be5d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
